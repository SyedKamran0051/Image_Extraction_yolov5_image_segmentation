{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "{'predictions': [{'x': 226.5, 'y': 295.0, 'width': 221.0, 'height': 324.0, 'confidence': 0.9870283007621765, 'class': 'crop', 'points': [{'x': 220.0, 'y': 456.0036592371468}, {'x': 122.0, 'y': 456.43330461380646}, {'x': 118.0, 'y': 456.01970196181685}, {'x': 116.99307419114608, 'y': 453.0}, {'x': 115.89189040530638, 'y': 134.0}, {'x': 334.0, 'y': 133.8951020020091}, {'x': 336.059797806628, 'y': 455.0}, {'x': 220.0, 'y': 456.0036592371468}], 'image_path': 'E:\\\\segmentation_model\\\\datasets\\\\Image_extraction-2\\\\train\\\\images\\\\1064-flatalaska-4-_jpg.rf.e79d4bee51272da0f6979693851abb71.jpg', 'prediction_type': 'InstanceSegmentationModel'}, {'x': 501.0, 'y': 290.0, 'width': 224.0, 'height': 324.0, 'confidence': 0.9841536283493042, 'class': 'crop', 'points': [{'x': 578.0, 'y': 451.0113866456364}, {'x': 401.0, 'y': 451.4275356634615}, {'x': 394.9309374708835, 'y': 450.0}, {'x': 391.99764616468275, 'y': 386.0}, {'x': 391.7184281189593, 'y': 320.0}, {'x': 388.88996441345967, 'y': 285.0}, {'x': 389.0, 'y': 128.9867542914717}, {'x': 422.0, 'y': 127.79903096716757}, {'x': 608.0260585011932, 'y': 128.0}, {'x': 611.0040342166922, 'y': 175.0}, {'x': 613.3614406500183, 'y': 446.0}, {'x': 609.0, 'y': 449.020240116861}, {'x': 578.0, 'y': 451.0113866456364}], 'image_path': 'E:\\\\segmentation_model\\\\datasets\\\\Image_extraction-2\\\\train\\\\images\\\\1064-flatalaska-4-_jpg.rf.e79d4bee51272da0f6979693851abb71.jpg', 'prediction_type': 'InstanceSegmentationModel'}], 'image': {'width': '640', 'height': '640'}}\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "from dotenv import load_dotenv\n",
    "!pip install python-dotenv\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "rf = Roboflow(api_key=api_key)\n",
    "project = rf.workspace().project(\"image_extraction-6dog0\")\n",
    "model = project.version(3).model\n",
    "\n",
    "# infer on a local image\n",
    "print(model.predict(\"E:\\segmentation_model\\datasets\\Image_extraction-2\\\\train\\images\\\\1064-flatalaska-4-_jpg.rf.e79d4bee51272da0f6979693851abb71.jpg\").json())\n",
    "\n",
    "# infer on an image hosted elsewhere\n",
    "#print(model.predict(\"URL_OF_YOUR_IMAGE\").json())\n",
    "\n",
    "# save an image annotated with your predictions\n",
    "model.predict(\"E:\\segmentation_model\\datasets\\Image_extraction-2\\\\train\\images\\\\1064-flatalaska-4-_jpg.rf.e79d4bee51272da0f6979693851abb71.jpg\").save(\"prediction.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Perform prediction\n",
    "image_path = \"E:\\segmentation_model\\datasets\\Image_extraction-2\\\\train\\images\\\\1064-flatalaska-4-_jpg.rf.e79d4bee51272da0f6979693851abb71.jpg\"\n",
    "response = model.predict(image_path).json()\n",
    "\n",
    "# Access the predictions directly\n",
    "predictions = response['predictions']\n",
    "\n",
    "for idx, prediction in enumerate(predictions):\n",
    "    # Load the original image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Extract the points for the segmentation mask\n",
    "    points = [(p['x'], p['y']) for p in prediction['points']]\n",
    "\n",
    "    # Calculate the bounding box coordinates\n",
    "    x_min = int(min(points, key=lambda p: p[0])[0])\n",
    "    x_max = int(max(points, key=lambda p: p[0])[0])\n",
    "    y_min = int(min(points, key=lambda p: p[1])[1])\n",
    "    y_max = int(max(points, key=lambda p: p[1])[1])\n",
    "\n",
    "    # Crop the segmented image based on the bounding box coordinates\n",
    "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    # Save the segmented image\n",
    "    save_path = f\"segmented_image_{idx}.jpg\"\n",
    "    cv2.imwrite(save_path, cropped_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Perform prediction\n",
    "image_path = \"datasets\\\\Image_extraction-2\\\\valid\\\\images\\\\flat198_jpg.rf.48ce81ce13708ccd9bb08d4c4d4c96ad.jpg\"\n",
    "response = model.predict(image_path).json()\n",
    "\n",
    "# Access the predictions directly\n",
    "predictions = response['predictions']\n",
    "\n",
    "for idx, prediction in enumerate(predictions):\n",
    "    # Load the original image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Extract the points for the segmentation mask\n",
    "    points = [(p['x'], p['y']) for p in prediction['points']]\n",
    "\n",
    "    # Calculate the bounding box coordinates\n",
    "    x_min = int(min(points, key=lambda p: p[0])[0])\n",
    "    x_max = int(max(points, key=lambda p: p[0])[0])\n",
    "    y_min = int(min(points, key=lambda p: p[1])[1])\n",
    "    y_max = int(max(points, key=lambda p: p[1])[1])\n",
    "\n",
    "    # Crop the segmented image based on the bounding box coordinates\n",
    "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    # Save the segmented image with original quality\n",
    "    save_path = f\"segmented_image_{idx}.jpg\"\n",
    "    cv2.imwrite(save_path, cropped_image, [cv2.IMWRITE_JPEG_QUALITY, 100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
